{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Machine Learning: Clustering (Zillow)\n",
    "#### Corey Solitaire\n",
    "#### 10/09/2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import acquire\n",
    "import prepare\n",
    "import wrangle_zillow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer, PowerTransformer, RobustScaler, MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.formula.api import ols\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score\n",
    "from sklearn.feature_selection import f_regression, SelectKBest, RFE \n",
    "from sklearn.linear_model import LinearRegression, LassoLars, TweedieRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire & Summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Acquire data from mySQL using the python module to connect and query. You will want to end with a single dataframe. Make sure to include: the logerror, all fields related to the properties that are available. You will end up using all the tables in the database.\n",
    "\n",
    "    - Be sure to do the correct join (inner, outer, etc.). We do not want to eliminate properties purely because they may have a null value for airconditioningtypeid.\n",
    "    - Only include properties with a transaction in 2017, and include only the last transaction for each properity (so no duplicate property ID's), along with zestimate error and date of transaction.\n",
    "    - Only include properties that include a latitude and longitude value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acquire df\n",
    "df = acquire.get_zillow_data(cached=False)\n",
    "#pd.set_option('display.max_rows', df.shape[0]+1) # Shows all the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77381, 70)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-8ff76cbeae0b>, line 26)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-8ff76cbeae0b>\"\u001b[0;36m, line \u001b[0;32m26\u001b[0m\n\u001b[0;31m    df = df.drop(df.index[[77380], inplace = True]\u001b[0m\n\u001b[0m                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Rename duplicate column id/id with id_delete/id\n",
    "def rename_columns(df):\n",
    "    df.columns = ['parcelid', 'typeconstructiontypeid', 'storytypeid',\n",
    "           'propertylandusetypeid', 'heatingorsystemtypeid', 'buildingclasstypeid',\n",
    "           'architecturalstyletypeid', 'airconditioningtypeid', 'id_delete',\n",
    "           'basementsqft', 'bathroomcnt', 'bedroomcnt', 'buildingqualitytypeid',\n",
    "           'calculatedbathnbr', 'decktypeid', 'finishedfloor1squarefeet',\n",
    "           'calculatedfinishedsquarefeet', 'finishedsquarefeet12',\n",
    "           'finishedsquarefeet13', 'finishedsquarefeet15', 'finishedsquarefeet50',\n",
    "           'finishedsquarefeet6', 'fips', 'fireplacecnt', 'fullbathcnt',\n",
    "           'garagecarcnt', 'garagetotalsqft', 'hashottuborspa', 'latitude',\n",
    "           'longitude', 'lotsizesquarefeet', 'poolcnt', 'poolsizesum',\n",
    "           'pooltypeid10', 'pooltypeid2', 'pooltypeid7',\n",
    "           'propertycountylandusecode', 'propertyzoningdesc',\n",
    "           'rawcensustractandblock', 'regionidcity', 'regionidcounty',\n",
    "           'regionidneighborhood', 'regionidzip', 'roomcnt', 'threequarterbathnbr',\n",
    "           'unitcnt', 'yardbuildingsqft17', 'yardbuildingsqft26', 'yearbuilt',\n",
    "           'numberofstories', 'fireplaceflag', 'structuretaxvaluedollarcnt',\n",
    "           'taxvaluedollarcnt', 'assessmentyear', 'landtaxvaluedollarcnt',\n",
    "           'taxamount', 'taxdelinquencyflag', 'taxdelinquencyyear',\n",
    "           'censustractandblock', 'id', 'logerror', 'pid', 'tdate',\n",
    "           'airconditioningdesc', 'architecturalstyledesc', 'buildingclassdesc',\n",
    "           'heatingorsystemdesc', 'propertylandusedesc', 'storydesc',\n",
    "           'typeconstructiondesc']\n",
    "    df.drop(columns = ['id_delete','pid'], inplace = True)\n",
    "    df = df.drop(df.index[[77380], inplace = True]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_columns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete duplicate columns id_delete and pid (dupilcate of parcelid)\n",
    "#df.drop(columns = ['id_delete','pid'], inplace = True)\n",
    "#df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 2. Summarize your data (summary stats, info, dtypes, shape, distributions, value_counts, etc.)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_summary(df):\n",
    "    print('The shape of the df:') \n",
    "    print(df.shape)  # df shape (row/column)\n",
    "    print('\\n')\n",
    "    print('Columns, Non-Null Count, Data Type:')\n",
    "    print(df.info())      # Column, Non Null Count, Data Type\n",
    "    print('\\n')\n",
    "    print('Summary statistics for the df:') \n",
    "    print(df.describe())             # Summary Statistics on Numeric Data Types\n",
    "    print('\\n')\n",
    "    print('Number of NaN values per column:') \n",
    "    print(df.isna().sum())           # NaN by column\n",
    "    print('\\n')\n",
    "    print('Number of NaN values per row:')  \n",
    "    print(df.isnull().sum(axis=1))   # NaN by row\n",
    "    print('\\n')\n",
    "    print('Value Counts per Column:')\n",
    "    for col in df.columns:\n",
    "        print('-' * 40 + col + '-' * 40 , end=' - ')\n",
    "        display(df[col].value_counts(dropna=False).head(10))\n",
    "        #display(df_resp[col].value_counts())  # Displays all Values, not just First 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 3. Write a function that takes in a dataframe of observations and attributes and returns a dataframe where each row is an atttribute name, the first column is the number of rows with missing values for that attribute, and the second column is percent of total rows that have missing values for that attribute. Run the function and document takeaways from this on how you want to handle missing values.*\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_and_percent_missing_row(df):\n",
    "    percent_missing = df.isnull().sum() * 100 / len(df)\n",
    "    total_missing = df.isnull().sum()\n",
    "    missing_value_df = pd.DataFrame({'num_rows_missing': total_missing,\n",
    "                                     'pct_rows_missing': percent_missing})\n",
    "    return missing_value_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_and_percent_missing_row(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "#### Based on this I would want to delete attributes that have less than 50% of their data missing.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### 4. Write a function that takes in a dataframe and returns a dataframe with 3 columns: the number of columns missing, percent of columns missing, and number of rows with n columns missing. Run the function and document takeaways from this on how you want to handle missing values.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def count_and_percent_missing_column(df):\n",
    "    num_rows = df.loc[:].isnull().sum()\n",
    "    num_cols_missing = df.loc[:, df.isna().any()].count()\n",
    "    pct_cols_missing = round(df.loc[:, df.isna().any()].count() / len(df.index) * 100, 3)\n",
    "    missing_cols_and_rows_df = pd.DataFrame({'num_cols_missing': num_cols_missing,\n",
    "                                             'pct_cols_missing': pct_cols_missing,\n",
    "                                             'num_rows': num_rows})\n",
    "    missing_cols_and_rows_df = missing_cols_and_rows_df.fillna(0)\n",
    "    missing_cols_and_rows_df['num_cols_missing'] = missing_cols_and_rows_df['num_cols_missing'].astype(int)\n",
    "    return missing_cols_and_rows_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_and_percent_missing_column(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Based on this I would want to delete columns that have less than 75% of their data missing.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare\n",
    "\n",
    "### 1. Remove any properties that are likely to be something other than single unit properties. (e.g. no duplexes, no land/lot, ...). There are multiple ways to estimate that a property is a single unit, and there is not a single \"right\" answer. But for this exercise, do not purely filter by unitcnt as we did previously. Add some new logic that will reduce the number of properties that are falsely removed. You might want to use # bedrooms, square feet, unit type or the like to then identify those with unitcnt not defined.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Property Codes that Fit the Breif of (Single Unit Properties)\n",
    "\n",
    "propertylandusetypeid\n",
    "-        261          Single Family Residential\n",
    "-        263          Mobile Home\n",
    "-        264          Townhouse\n",
    "-        266          Condominium\n",
    "-        273          Bungalow\n",
    "-        276          Patio Home\n",
    "-        275          Manufactured, Modular, Prefabricated Homes\n",
    "-        279          Inferred Single Family Residential\n",
    "\n",
    "Alternate Methods to determine single Unit Properties\n",
    "\n",
    "df.unitcnt == 1   \n",
    "df.roomcnt > 0   \n",
    "df.garagecarcnt > 0   \n",
    "df.bedroomcnt > 0   \n",
    "df.bathroomcnt > 0   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = acquire.get_zillow_data(cached=True)\n",
    "unit_df = df[df['unitcnt']==1]\n",
    "unit_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = acquire.get_zillow_data(cached=True)\n",
    "room_df = df[df['roomcnt']>0]\n",
    "room_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = acquire.get_zillow_data(cached=True)\n",
    "garage_df = df[df['garagecarcnt']>0]\n",
    "garage_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = acquire.get_zillow_data(cached=True)\n",
    "bed_df = df[df['bedroomcnt']>0]\n",
    "bed_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = acquire.get_zillow_data(cached=True)\n",
    "bath_df = df[df['bathroomcnt']>0]\n",
    "bath_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique dataframe that does not use proptertylandusetypeid to identify single unit properties\n",
    "seconday_featutes_df = pd.concat([bed_df, bath_df, garage_df, room_df, unit_df]).drop_duplicates('id').reset_index(drop=True)\n",
    "seconday_featutes_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_rows', df.shape[0]+1) # Shows all the rows\n",
    "#new_df.parcelid.value_counts(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique dataframe tha uses propterylandusetypeid to identify single unit properties\n",
    "df = acquire.get_zillow_data(cached=True)\n",
    "p261_df = df[df['propertylandusetypeid'] == 261]\n",
    "p263_df = df[df['propertylandusetypeid'] == 263]\n",
    "p264_df = df[df['propertylandusetypeid'] == 264]    \n",
    "p266_df = df[df['propertylandusetypeid'] == 266] \n",
    "p273_df = df[df['propertylandusetypeid'] == 273]\n",
    "p275_df = df[df['propertylandusetypeid'] == 275]\n",
    "p276_df = df[df['propertylandusetypeid'] == 276]    \n",
    "p279_df = df[df['propertylandusetypeid'] == 279]\n",
    "property_df = pd.concat([p261_df, p263_df, p264_df, p266_df, p273_df, p275_df, p276_df, p279_df]).drop_duplicates('id').reset_index(drop=True)\n",
    "property_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined Dataframe, seconday_features_df and verified property_df\n",
    "all_single_unit_properties_df = pd.concat([seconday_featutes_df, property_df]).drop_duplicates('id').reset_index(drop=True)\n",
    "all_single_unit_properties_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicated rows\n",
    "duplicate = all_single_unit_properties_df[all_single_unit_properties_df.duplicated(keep = 'last')]   \n",
    "print(\"Duplicate Rows :\") \n",
    "duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df = pd.read_csv('zillow_df.csv')\n",
    "unit_df = df[df['unitcnt']==1]\n",
    "df = df = pd.read_csv('zillow_df.csv')\n",
    "room_df = df[df['roomcnt']>0]\n",
    "df = df = pd.read_csv('zillow_df.csv')\n",
    "garage_df = df[df['garagecarcnt']>0]\n",
    "df = df = pd.read_csv('zillow_df.csv')\n",
    "bed_df = df[df['bedroomcnt']>0]\n",
    "df = df = pd.read_csv('zillow_df.csv')\n",
    "bath_df = df[df['bathroomcnt']>0]\n",
    "df = df = pd.read_csv('zillow_df.csv')\n",
    "#seconday_featutes_df = pd.concat([bed_df, bath_df, garage_df, room_df, unit_df]).drop_duplicates('id').reset_index(drop=True)\n",
    "df = df = pd.read_csv('zillow_df.csv')\n",
    "p261_df = df[df['propertylandusetypeid'] == 261]\n",
    "df = df = pd.read_csv('zillow_df.csv')\n",
    "p263_df = df[df['propertylandusetypeid'] == 263]\n",
    "df = df = pd.read_csv('zillow_df.csv')\n",
    "p264_df = df[df['propertylandusetypeid'] == 264]    \n",
    "df = df = pd.read_csv('zillow_df.csv')\n",
    "p266_df = df[df['propertylandusetypeid'] == 266] \n",
    "df = df = pd.read_csv('zillow_df.csv')\n",
    "p273_df = df[df['propertylandusetypeid'] == 273]\n",
    "df = df = pd.read_csv('zillow_df.csv')\n",
    "p275_df = df[df['propertylandusetypeid'] == 275]\n",
    "df = df = pd.read_csv('zillow_df.csv')\n",
    "p276_df = df[df['propertylandusetypeid'] == 276]    \n",
    "df = df = pd.read_csv('zillow_df.csv')\n",
    "p279_df = df[df['propertylandusetypeid'] == 279]\n",
    "complete_df = pd.concat([p261_df, p263_df, p264_df, p266_df, p273_df, p275_df, p276_df, p279_df,bed_df, bath_df, garage_df, room_df, unit_df]).drop_duplicates('id').reset_index(drop=True)\n",
    "df = pd.concat([complete_df]).drop_duplicates('id').reset_index(drop=True)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "duplicate = df[df.duplicated(keep = 'last')]   \n",
    "print(\"Duplicate Rows :\") \n",
    "duplicate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### This suggests that filtering by propertylandusetypeid over filtered the dataframe.  By combining the two filters wer were able to add 6,000 more records to our study\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a function that will drop rows or columns based on the percent of values that are missing: handle_missing_values(df, prop_required_column, prop_required_row).\n",
    "\n",
    "***\n",
    "\n",
    "**The input:**   \n",
    "\n",
    " - A dataframe\n",
    " - A number between 0 and 1 that represents the proportion, for each column, of rows with non-missing values required to keep the column. i.e. if prop_required_column = .6, then you are requiring a column to have at least 60% of values not-NA (no more than 40% missing).\n",
    " - A number between 0 and 1 that represents the proportion, for each row, of columns/variables with non-missing values required to keep the row. For example, if prop_required_row = .75, then you are requiring a row to have at least 75% of variables with a non-missing value (no more that 25% missing).\n",
    "        \n",
    "        \n",
    "**The output:**   \n",
    "\n",
    "- The dataframe with the columns and rows dropped as indicated. Be sure to drop the columns prior to the rows in your function.\n",
    "       \n",
    "- hint:\n",
    "  - Look up the dropna documentation.\n",
    "  - You will want to compute a threshold from your input values (prop_required) and total number of rows or columns.\n",
    "  - Make use of inplace, i.e. inplace=True/False.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep Data Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Prep Data\n",
    "def data_prep(df, cols_to_remove=[], prop_required_column=.5, prop_required_row=.75):\n",
    "    \n",
    "    def remove_columns(df, cols_to_remove):  \n",
    "        df = df.drop(columns=cols_to_remove)\n",
    "        return df\n",
    "\n",
    "    def handle_missing_values(df, prop_required_column = .5, prop_required_row = .75):\n",
    "        threshold = int(round(prop_required_column*len(df.index),0))\n",
    "        df.dropna(axis=1, thresh=threshold, inplace=True)\n",
    "        threshold = int(round(prop_required_row*len(df.columns),0))\n",
    "        df.dropna(axis=0, thresh=threshold, inplace=True)\n",
    "        return df\n",
    "    \n",
    "    df = remove_columns(df, cols_to_remove)  # Removes Specified Columns\n",
    "    df = handle_missing_values(df, prop_required_column, prop_required_row) # Removes Specified Rows\n",
    "    #df.dropna(inplace=True) # Drops all Null Values From Dataframe\n",
    "    return df\n",
    "\n",
    "\n",
    "# How to Call the Function\n",
    "# df = prepare.data_prep(\n",
    "#     df,\n",
    "#     cols_to_remove=[],\n",
    "#     prop_required_column=.6,\n",
    "#     prop_required_row=.75\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prepare.data_prep(\n",
    "    all_single_unit_properties_df,\n",
    "    cols_to_remove=[],\n",
    "    prop_required_column=.6,\n",
    "    prop_required_row=.75\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Decide how to handle the remaining missing values:\n",
    "\n",
    "- Fill with constant value.\n",
    "- Impute with mean, median, mode.\n",
    "- Drop row/column\n",
    "\n",
    "*** For simplicitys sake I will choose to drop the remaning NaN values, and will do that in the function.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Add Above Functions to File Called wrangle_zillow.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_summary(provides df info before wrangle function)\n",
    "df = df = pd.read_csv('zillow_df.csv')\n",
    "wrangle_zillow.df_summary(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing wrangle_zillow Function (Splits/Scales)\n",
    "\n",
    "df, X_train_explore, X_train_scaled, X_validate_scaled, X_test_scaled = wrangle_zillow.wrangle_zillow(acquire.get_zillow_data(cached=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape, X_train_explore.shape, X_train_scaled.shape, X_validate_scaled.shape, X_test_scaled.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing IQR Function\n",
    "wrangle_zillow.quartiles_and_outliers(X_train_explore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing and Determining IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Data\n",
    "df.hist(figsize=(24, 10), bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quartiles_and_outliers(df):\n",
    "    def get_upper_outliers(s, k):\n",
    "        '''\n",
    "        Given a series and a cutoff value, k, returns the upper outliers for the\n",
    "        series.\n",
    "\n",
    "        The values returned will be either 0 (if the point is not an outlier), or a\n",
    "        number that indicates how far away from the upper bound the observation is.\n",
    "        '''\n",
    "        q1, q3 = s.quantile([.25, .75])\n",
    "        iqr = q3 - q1\n",
    "        upper_bound = q3 + k * iqr\n",
    "        return s.apply(lambda x: max([x - upper_bound, 0]))\n",
    "\n",
    "    def add_upper_outlier_columns(df, k):\n",
    "        '''\n",
    "        Add a column with the suffix _outliers for all the numeric columns\n",
    "        in the given dataframe.\n",
    "        '''\n",
    "        # outlier_cols = {col + '_outliers': get_upper_outliers(df[col], k)\n",
    "        #                 for col in df.select_dtypes('number')}\n",
    "        # return df.assign(**outlier_cols)\n",
    "        for col in df.select_dtypes('number'):\n",
    "            df[col + '_outliers'] = get_upper_outliers(df[col], k)\n",
    "        return df\n",
    "\n",
    "    add_upper_outlier_columns(df, k=1.5)    \n",
    "    outlier_cols = [col for col in df if col.endswith('_outliers')]\n",
    "    for col in outlier_cols:\n",
    "        print('~~~\\n' + col)\n",
    "        data = df[col][df[col] > 0]\n",
    "        print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quartiles_and_outliers(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
